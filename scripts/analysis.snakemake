#!/usr/bin/env python3

## snakemake workflow for analysis of pingtao data in datahog at
## /tsl/data/reads/jjones/dingp_ath_atacseq_2019_1/

import os
from glob import glob

configfile: "config.yaml"

projectdir = config["projectdir"]
datafolder = config["datafolder"]
reference = config["reference"]

def get_samplenames(datafolder):
    ## get the sample name from datahog path

    samplenames = set()
    for path in glob(datafolder + "/*/*/raw"):
        sample = path.split("/")[-3]
        #if sample.startswith("wt"):
        samplenames.add(sample)


    return samplenames


samplenames = get_samplenames(datafolder)

print(samplenames)

sample_data_fastq_files=[]
sampleData = dict()
samplefiles = dict()
for current_sample in samplenames:

    samplefiles[current_sample] = {}
    sampleData[current_sample] = []
    dataset_name = glob_wildcards(datafolder + current_sample + "/{dataset}/raw").dataset
    for dataset in dataset_name:
        # print(sample, dataset)
        sampleData[current_sample].append(dataset)
        R1 = glob(datafolder + current_sample + "/" + dataset + "/raw/*_R1.*")
        R2 = glob(datafolder + current_sample + "/" + dataset + "/raw/*_R2.*")
        samplefiles[current_sample][dataset] = {"R1": R1, "R2": R2}
        sampleData[current_sample].append([R1, R2])
        sample_data_fastq_files += glob(datafolder + current_sample + "/" + dataset + "/raw/*_R1.*")
        sample_data_fastq_files += glob(datafolder  + current_sample + "/" + dataset + "/raw/*_R2.*")

def get_samplename(fastq):
    sample = fastq.split("/")[-4]
    return sample
def get_dataset_name(fastq):
    dataset = fastq.split("/")[-3]
    return dataset
def get_data_name(fastq):
    data = os.path.basename(fastq).split(".")[0]
    return data
def get_data_name_without_strand(fastq):
    data = os.path.basename(fastq).split(".")[0].split("_")[0]
    return data
def get_sample_from_bam(bam):
    sample = "_".join(os.path.basename(bam).split("_")[:3])
    print(sample)
    return sample
def get_data_from_bam(bam):
    data = os.path.basename(bam).split("_")[3]
    print(data)
    return data
# print(samplefiles)
print(sample_data_fastq_files)
print(sampleData)
rule fastqc:
    input:
        datafolder + "{sample}/{dataset}/raw/{data}.fastq.gz"
    output:
        projectdir + "results/fastqc/{sample}/{dataset}/{data}_fastqc.html"
    # log: projectdir + "logs/fastqc/{sample}/fastqc.log"
    shell: "fastqc --extract -f fastq -o " + projectdir + "results/fastqc/{wildcards.sample}/{wildcards.dataset} {input}"

rule run_fastqc:
    input: [projectdir + "results/fastqc/" + get_samplename(fastq) + "/" + get_dataset_name(fastq) + "/" + get_data_name(fastq) + "_fastqc.html"  for fastq in sample_data_fastq_files]

rule get_fastqc_report:
    input: [projectdir + "results/fastqc/" + get_samplename(fastq) + "/" + get_dataset_name(fastq) + "/" + get_data_name(fastq) + "_fastqc.html"  for fastq in sample_data_fastq_files]
    output: projectdir + "results/fastqc/report.txt"

    shell: "bash scripts/get_fastqc_report.sh " + projectdir + "results/fastqc > {output}"

rule index_referece:
    input: reference
    output:
        reference + ".1.bt2",
        reference + ".2.bt2",
        reference + ".3.bt2",
        reference + ".4.bt2",
        reference + ".rev.1.bt2",
        reference + ".rev.2.bt2"
    shell: "bowtie-build -f {input}"

rule align_reads:
    input:
        R1 = lambda wildcards: samplefiles[wildcards.dataset][wildcards.dataset]['R1'],
        R2 = lambda wildcards: samplefiles[wildcards.dataset][wildcards.dataset]['R2'],
        index = [reference + ".1.bt2",
        reference + ".2.bt2",
        reference + ".3.bt2",
        reference + ".4.bt2",
        reference + ".rev.1.bt2",
        reference + ".rev.2.bt2"]
    output: projectdir + "results/alignment/{dataset}_sorted.bam"
    threads: 16
    resources: mem_mb = lambda wildcards, attempt: attempt * 300000
    benchmark: projectdir + "benchmark/alignment/{dataset}.txt"
    log: projectdir + "logs/alignment/{dataset}.log"
    shell: "bowtie2 --threads {threads} -x {reference} --very-sensitive --end-to-end --maxins 20000 --no-discordant --no-mixed --fr --time --no-unal --qc-filter -1 {input.R1} -2 {input.R2} | samtools view -b --reference {reference} --threads {threads} | samtools sort -o {output} && samtools index {output}"

rule run_alignment:
    input:
        [ projectdir + "results/alignment/" + data[0] + "_sorted.bam" for data in sampleData.values()]

rule picard_duplication:
    input: "/tsl/scratch/shrestha/pingtao/dingp_Ath_ATACseq_2019_1/{sample}_{data}_sorted.bam"
    output:
        bam = projectdir + "results/picard_duplication/{sample}_{data}_marked_duplicates.bam",
        txt = projectdir + "results/picard_duplication/{sample}_{data}_dupmetrics.txt"
    # message: "Analysing duplication for sample file {wildcards.sample}"
    # log: projectdir + "logs/{sample}/picard_duplication.log"
    shell: "mkdir -p " + projectdir  + "results/picard_duplication" + " && picard MarkDuplicates I={input} O={output.bam} M={output.txt} REMOVE_SEQUENCING_DUPLICATES=false TAGGING_POLICY=OpticalOnly TAG_DUPLICATE_SET_MEMBERS=true "

rule check_duplication:
    input: [projectdir + "results/picard_duplication/" + get_samplename(fastq) + "_" +  get_data_name_without_strand(fastq) + "_marked_duplicates.bam" for fastq in sample_data_fastq_files]

rule get_duplication_table:
    input: [projectdir + "results/picard_duplication/" + get_samplename(fastq) + "_" +  get_data_name_without_strand(fastq) + "_marked_duplicates.bam" for fastq in sample_data_fastq_files]

    output: projectdir + "results/picard_duplication/duplication_percentage_table.txt"
    shell: "bash scripts/get_duplication_table.sh " + projectdir + "results/picard_duplication > {output}"


rule sard1_extreads:
    input: "/tsl/data/extReads/{srr}.fastq.gz"
    output: projectdir + "results/alignment/{srr}_sorted.bam"
    threads: 16
    resources: mem_mb = lambda wildcards, attempt: attempt * 300000
    benchmark: projectdir + "benchmark/alignment/{srr}.txt"
    log: projectdir + "logs/alignment/{srr}.log"
    shell: "bowtie2 --threads {threads} -x {reference} --sensitive --end-to-end --maxins 20000 --time --no-unal --qc-filter -U {input}  | samtools view -b --reference {reference} --threads {threads} | samtools sort -o {output} && samtools index {output}"

rule sard1_chipseq:
    input:
        treatment = projectdir + "results/alignment/SRR2776874_sorted.bam",
        control = projectdir + "results/alignment/SRR2776875_sorted.bam"
    output:
        outfiles = [projectdir + "results/chipseq/sard1_control_lambda.bdg",
            projectdir + "results/chipseq/sard1_cutoff_analysis.txt",
            projectdir + "results/chipseq/sard1_model.r",
            projectdir + "results/chipseq/sard1_peaks.narrowPeak",
            projectdir + "results/chipseq/sard1_peaks.xls",
            projectdir + "results/chipseq/sard1_summits.bed",
            projectdir + "results/chipseq/sard1_treat_pileup.bdg"]

    message: "Chipseq analysis"
    benchmark: projectdir + "benchmark/chipseq/sard1_chipseq.log"
    log: projectdir + "logs/chipseq/sard1_chipseq.log"
    shell: "macs2 callpeak -t {input.treatment} -c {input.control} -f BAM -g 135000000 --keep-dup all --outdir {projectdir}/results/chipseq --name sard1 --bdg --verbose 2 --trackline --d-min 30 --call-summits --fe-cutoff 5 --cutoff-analysis --tsize 42 --pvalue 0.01 --min-length 10 --max-gap 10 --format BAM --scale-to large > {log} "

rule run_sard1_extreads:
    input: expand([projectdir + "results/alignment/{srr}_sorted.bam"], srr=["SRR2776874", "SRR2776875"])
